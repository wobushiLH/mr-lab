<configuration>
<property>
<name>dfs.namenode.name.dir</name>
<value>file:/home/hadoop/hdfs/hdname</value>
</property>
<property>
<name>dfs.datanode.data.dir</name>
<value>file:/services1/hddata,file:/services2/hddata</value>
</property>

<property>
<name>dfs.namenode.handler.count</name>
<value>30</value>
</property>

<property>
<name>dfs.replication</name>
<value>2</value>
</property>

<property>
<name>dfs.support.append</name>
<value>true</value>
<description>Allow appends to files.</description>
</property>

<property>
<name>dfs.block.size</name>
<value>134217728</value>
<description>The default block size for new files.</description>
</property>

<property>
<name>io.file.buffer.size</name>
<value>131072</value>
</property>

<property>
<name>io.bytes.per.checksum</name>
<value>1024</value>
</property>

<property>
<name>dfs.balance.bandwidthPerSec</name>
<value>10485760</value>
</property>

<property>
<name>dfs.datanode.max.xcievers</name>
<value>4096</value>
</property>

<property>
<name>dfs.permissions</name>
<value>false</value>
</property>

<property>
<name>dfs.heartbeat.interval</name>
<value>5</value>
<description>Determines datanode heartbeat interval in seconds.</description>
</property>
<property>
<name>dfs.heartbeat.recheck.interval</name>
<value>30000</value>
<description>Determines when machines are marked dead 单位：毫秒！！！</description>
</property>

<property>
<name>dfs.namenode.stale.datanode.interval</name>
<value>30000</value>
</property>
<property>
<name>dfs.namenode.check.stale.datanode</name>
<value>true</value>
</property>

<property>
<name>dfs.datanode.du.reserved</name>
<value>53687091200</value>
<description>disk free 50G</description>
</property>




<property>
<name>dfs.nameservices</name>
<value>hacluster</value>
</property>

<property>
<name>dfs.ha.namenodes.hacluster</name>
<value>nm1.hdfs,ns1.hdfs</value>
</property>

<property>
<name>dfs.namenode.rpc-address.hacluster.nm1.hdfs</name>
<value>nm1.hdfs:8020</value>
</property>
<property>
<name>dfs.namenode.rpc-address.hacluster.ns1.hdfs</name>
<value>ns1.hdfs:8020</value>
</property>

<property>
<name>dfs.namenode.http-address.hacluster.nm1.hdfs</name>
<value>nm1.hdfs:50070</value>
</property>
<property>
<name>dfs.namenode.http-address.hacluster.ns1.hdfs</name>
<value>ns1.hdfs:50070</value>
</property>

<property>
<name>dfs.namenode.shared.edits.dir</name>
<value>qjournal://nm1.hdfs:8485;ns1.hdfs:8485;dn127.hdfs:8485/hacluster</value>
</property>

<property>
 <name>dfs.journalnode.edits.dir</name>
 <value>/home/hadoop/hdfs/journaledits</value>
</property>

<property>
<name>dfs.client.failover.proxy.provider.hacluster</name>
<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>

<property>
<name>dfs.ha.fencing.methods</name>
<value>sshfence</value>
</property>

<property>
<name>dfs.ha.fencing.ssh.private-key-files</name>
<value>/home/hadoop/.ssh/id_rsa</value>
</property>

<property>
<name>dfs.ha.fencing.ssh.connect-timeout</name>
<value>30000</value>
</property>

<property>
<name>dfs.ha.automatic-failover.enabled</name>
<value>true</value>
</property>
<property>
<name>ha.zookeeper.session-timeout.ms</name>
<value>60000</value>
<description>ms</description>
</property>

<property>
<name>dfs.hosts.exclude</name>
<value>/home/hadoop/hdfs/hadoop-2.3.0-cdh5.1.0/etc/hadoop/nn-excluded-list</value>
</property>
<!--
 <property>
    <name>dfs.checksum.type</name>
    <value>CRC32</value>
 </property>
-->

</configuration>
